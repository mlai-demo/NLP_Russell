{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2020 Almintas Povilaitis\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
    "\n",
    "https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "<td>\n",
    "<a target=\"_blank\"  href=\"https://colab.research.google.com/github/mlai-demo/NLP_Russell/blob/master/RussellPub.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "</td><td>\n",
    "<a target=\"_blank\"  href=\"https://github.com/mlai-demo/NLP_Russell/blob/master/RussellPub.ipynb\"><img width=32px src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a></td></table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m8CHZ1rbhynM",
    "outputId": "fda71818-3ae0-4154-da83-c291c5295304"
   },
   "outputs": [],
   "source": [
    "#adjusts the notebook look on the screen\n",
    "#no need to run in Colab which does a good job adjusting the notebook window\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(data=\"\"\"\n",
    "<style>\n",
    "    div#notebook-container    { width: 95%; }\n",
    "    div#menubar-container     { width: 95%; }\n",
    "    div#maintoolbar-container { width: 99%; }\n",
    "</style>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "0tjrM0FqiLVP",
    "outputId": "35c64d0b-7723-43f2-c070-ccb452af1e8f"
   },
   "outputs": [],
   "source": [
    "#set up Gutenberg package\n",
    "!sudo apt-get install libdb++-dev\n",
    "!export BERKELEYDB_DIR=/usr\n",
    "!pip install gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wxmTQqqVHTwh"
   },
   "outputs": [],
   "source": [
    "#add own stop words to those of Scikit-learn\n",
    "import sklearn.feature_extraction.text as text\n",
    "\n",
    "new_stop_words = ['one','came', 'come', 'upon', 'made','though', 'indeed', 'yet', 'without'\n",
    "                 'thus','therefore', 'another', 'much', 'many', 'either', 'upon', 'would',\n",
    "                     'around', 'without', 'when', 'also', 'could', 'say', 'sent', 'notwithstanding', 'hence', 'thus',\n",
    "                     'bertrand', 'russell']\n",
    "my_stop_words = text.ENGLISH_STOP_WORDS.union(new_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "99AOFML5hynX",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#import texts from Gutenberg\n",
    "import os\n",
    "from gutenberg.acquire import load_etext\n",
    "from gutenberg.cleanup import strip_headers\n",
    "\n",
    "path = os.getcwd()\n",
    "text_list = [5827, 690, 2529, 25447, 4776, 44932, 37090, 17350, 55610, 52091]\n",
    "\n",
    "#write all into one file in the TextsPub directory\n",
    "os.mkdir(path + '/TextsPub')\n",
    "with open(path + '/TextsPub/Russell.txt', 'w') as f:\n",
    "    for text in text_list:\n",
    "        text = strip_headers(load_etext(text)).strip()\n",
    "        f.write(text)\n",
    "        \n",
    "#write texts into separate files in the TextsPub directory\n",
    "for text in text_list:\n",
    "    with open(f\"{path+'/TextsPub'}/{text}\", \"w\") as f:\n",
    "        f.write(strip_headers(load_etext(text)).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 680
    },
    "colab_type": "code",
    "id": "Z25JMT6yhynd",
    "outputId": "a9736272-b044-497a-90b2-1297ca1fa1ec"
   },
   "outputs": [],
   "source": [
    "#tonekize the text and plot by word frequency\n",
    "\n",
    "#import os #use only if cell above is not run first\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt') #if using nltk for the first time or using Colab\n",
    "nltk.download('stopwords') #if using nltk for the first time or using Colab\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "path = os.getcwd()\n",
    "\n",
    "no_short = re.compile(r'\\W*\\b\\w{1,2}\\b')\n",
    "with open(path + '/TextsPub/Russell.txt') as f, open(path + '/TextsPub/Russell_tokens.txt', 'w') as out_f:\n",
    "    text = f.read()\n",
    "    for line in f:                                            #remove xtra empty lines\n",
    "        if not line.strip(): continue  # skip the empty line\n",
    "        outfile.write(line)\n",
    "    text = no_short.sub('', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    stop_words = my_stop_words\n",
    "    words = [w for w in words if not w in my_stop_words]\n",
    "    new_text = ' '.join(words)\n",
    "    plt.figure(figsize=(18, 9))\n",
    "    fd = nltk.FreqDist(words)\n",
    "    fd.plot(40,title = \"40 Most Frequent Words\", cumulative=False)\n",
    "    #print(new_text[:500])\n",
    "    out_f.write(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "YOiburc4hynk",
    "outputId": "f2aebaed-8131-4e37-a99d-35426b0a5749"
   },
   "outputs": [],
   "source": [
    "#count all and unique words\n",
    "\n",
    "unique = set(words)\n",
    "print(\"The text is {} words long and {} unique words\".format(len(words), len(unique)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "BrBh5x0ghyno",
    "outputId": "57ed3645-c81e-43e0-aa15-ba417cbaa2a9"
   },
   "outputs": [],
   "source": [
    "#lemmatize text\n",
    "\n",
    "nltk.download('wordnet') #if using nltk for the first time or using Colab\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "with open(path + '/TextsPub/Russell_tokens.txt') as f, open(path + '/TextsPub/Russell_lemma.txt', 'w') as out_f:\n",
    "    text = f.read()\n",
    "    tokens = word_tokenize(text)\n",
    "    lemma = WordNetLemmatizer()\n",
    "    lemmed = [lemma.lemmatize(word) for word in tokens]\n",
    "    #print(lemmed[:100])\n",
    "    new_lem_text = ' '.join(lemmed)\n",
    "    out_f.write(new_lem_text)\n",
    "    \n",
    "unique_lem = set(lemmed)\n",
    "print(\"The lemmatized text is {} words long and {} unique words\".format(len(lemmed), len(unique_lem)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 766
    },
    "colab_type": "code",
    "id": "lYYlGlohhyny",
    "outputId": "0e32d981-134c-4838-c618-fb5ff42c148b"
   },
   "outputs": [],
   "source": [
    "#draw word cloud\n",
    "\n",
    "import pandas as pd\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt  \n",
    "\n",
    "stemmed_text = open('TextsPub/Russell_lemma.txt').read()\n",
    "\n",
    "wordcloud = WordCloud(stopwords=my_stop_words,\n",
    "                      max_font_size=400,\n",
    "                      width=2500,\n",
    "                      height=2000,\n",
    "                      random_state=64,\n",
    "                     ).generate(stemmed_text)\n",
    "fig = plt.figure().set_size_inches(16, 16)\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title('Bertrand Russell Wordcloud', fontsize= 20)\n",
    "plt.xlabel(\"size\", fontsize= 12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "nJsKoal1hyn2",
    "outputId": "d6854833-94c9-4fbc-e887-519c37d5edd1"
   },
   "outputs": [],
   "source": [
    "#count lemmatized words and sort in decreasing order\n",
    "\n",
    "lemmed_text = open('TextsPub/Russell_lemma.txt').read()\n",
    "\n",
    "BagLems = {}\n",
    "for w in lemmed_text.split(\" \"):\n",
    "    if w in BagLems:\n",
    "        BagLems[w]+=1\n",
    "    else:\n",
    "        BagLems[w]=1\n",
    "lemmed_data = sorted(BagLems.items(), key=lambda pair: pair[1], reverse=True)\n",
    "print(lemmed_data[:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sfOfSwzohyn6"
   },
   "outputs": [],
   "source": [
    "#rename files and move into a new directory\n",
    "\n",
    "import shutil\n",
    "\n",
    "os.mkdir('RenamedPub')\n",
    "\n",
    "old_txt_dir = path + \"/TextsPub\"\n",
    "new_txt_dir = path + \"/RenamedPub\"\n",
    "\n",
    "file01 = shutil.copy(old_txt_dir + '/5827', new_txt_dir + '/Problems_Philosophy.txt')\n",
    "file02 = shutil.copy(old_txt_dir + '/690', new_txt_dir + '/Roads_Freedom.txt')\n",
    "file03 = shutil.copy(old_txt_dir + '/2529', new_txt_dir + '/Analysis_Mind.txt')\n",
    "file04 = shutil.copy(old_txt_dir + '/25447', new_txt_dir + '/Mysticism_Logic.txt')\n",
    "file05 = shutil.copy(old_txt_dir + '/4776', new_txt_dir + '/Political_Ideals.txt')\n",
    "file06 = shutil.copy(old_txt_dir + '/44932', new_txt_dir + '/Free_Thought.txt')\n",
    "file07 = shutil.copy(old_txt_dir + '/37090', new_txt_dir + '/Knowledge.txt')\n",
    "file08 = shutil.copy(old_txt_dir + '/17350', new_txt_dir + '/Bolshevism.txt')\n",
    "file09 = shutil.copy(old_txt_dir + '/55610', new_txt_dir + '/Why_Fight.txt')\n",
    "file10 = shutil.copy(old_txt_dir + '/52091', new_txt_dir + '/Foundations_Geometry.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ieLmFgFHhyn_",
    "outputId": "5bbf338e-64f9-4f46-9ff0-32b7834ddbf2"
   },
   "outputs": [],
   "source": [
    "#count number of texts\n",
    "\n",
    "texts = sorted([os.path.join(new_txt_dir, fn) for fn in os.listdir(new_txt_dir)])\n",
    "text_number = len(texts); text_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "id": "kyJHXznOkFr4",
    "outputId": "09f8724e-cabf-418d-c174-cf933576d35c"
   },
   "outputs": [],
   "source": [
    "#list text titles in alphabetical order\n",
    "\n",
    "text_titles = []\n",
    "\n",
    "for fn in texts:\n",
    "    basename = os.path.basename(fn)\n",
    "    title, ext = os.path.splitext(basename)\n",
    "    text_titles.append(title)\n",
    "\n",
    "titles = sorted(set(text_titles))\n",
    "titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dABHNKYRhyoM"
   },
   "outputs": [],
   "source": [
    "#vectorize the text, create sparse and dense (numpy array) text matrices (tm), and the vocabulary\n",
    "\n",
    "import numpy as np  \n",
    "import sklearn.feature_extraction.text as text\n",
    "\n",
    "vectorizer = text.CountVectorizer(input='filename', stop_words=my_stop_words, min_df=text_number)\n",
    "tm_sparse = vectorizer.fit_transform(texts)\n",
    "tm_array = vectorizer.fit_transform(texts).toarray()\n",
    "vocab = np.array(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "SA6a5IVgHTxd",
    "outputId": "7f465e64-5d34-4473-ad11-04eaaa49252b"
   },
   "outputs": [],
   "source": [
    "tm_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ydha1idphyoS",
    "outputId": "280077f0-703f-49b2-8600-1556399bc5ea"
   },
   "outputs": [],
   "source": [
    "tm_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0U4e0YEIHTxn"
   },
   "outputs": [],
   "source": [
    "#run non-negative matrix factorization\n",
    "\n",
    "from sklearn import decomposition\n",
    "\n",
    "num_topics = 10\n",
    "\n",
    "deco_nmf = decomposition.NMF(n_components=num_topics, random_state=4, max_iter=1000, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "QeDgvKjcHTxr",
    "outputId": "c7e80b68-a030-41e3-8d96-d56198a61132"
   },
   "outputs": [],
   "source": [
    "deco_nmf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "-wMji4RCHTxt",
    "outputId": "2137074b-6b4b-48af-d2a7-c58825360343"
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "t0 = time()\n",
    "text_topic_nmf = deco_nmf.fit_transform(dtm_sparse)\n",
    "print(\"Done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hx3TfUY0HTxw"
   },
   "outputs": [],
   "source": [
    "num_top_words = 10\n",
    "topic_words_nmf = []\n",
    "\n",
    "for topic in deco_nmf.components_:\n",
    "    word_idx = np.argsort(topic)[::-1][0:num_top_words]\n",
    "    topic_words_nmf.append([vocab[i] for i in word_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "id": "TZST4lM6HTx3",
    "outputId": "f232fcc6-3eb5-43a8-c44d-78527760f223"
   },
   "outputs": [],
   "source": [
    "#list ten topics\n",
    "\n",
    "for t in range(len(topic_words_nmf)):\n",
    "    print(\"NMF Topic {}: {}\".format(t, ' '.join(topic_words_nmf[t][:10])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "id": "jpTKxlJ3HTx6",
    "outputId": "fd50e75b-e385-477e-bc79-efc031e4fa7c"
   },
   "outputs": [],
   "source": [
    "#top four topics sorted by text\n",
    "\n",
    "for i in range(len(text_topic_nmf)):\n",
    "    top_topics = np.argsort(text_topic_nmf[i,:])[::-1][0:4]\n",
    "    top_topics_str = ' '.join(str(t) for t in top_topics)\n",
    "    print(\"{}: {}\".format(titles[i], top_topics_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "id": "L6bTUTUBHTx8",
    "outputId": "145cade1-3b8d-49e0-be1e-9a5e4dcd90cb"
   },
   "outputs": [],
   "source": [
    "#all topics sorted by text\n",
    "\n",
    "for i in range(len(text_topic_nmf)):\n",
    "    top_topics = np.argsort(text_topic_nmf[i,:])[::-1]\n",
    "    top_topics_str = ' '.join(str(t) for t in top_topics)\n",
    "    print(\"{}: {}\".format(titles[i], top_topics_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "XhW6-duKhyoY",
    "outputId": "a096f3e2-fc4d-4a16-8d4e-fd08a3ea4a80"
   },
   "outputs": [],
   "source": [
    "deco_spca = decomposition.SparsePCA(n_components=num_topics, normalize_components='deprecated', random_state=4)\n",
    "deco_spca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ow8DBiNRhyof",
    "outputId": "37b7d86e-f657-494a-9cd4-27ccca0395ae"
   },
   "outputs": [],
   "source": [
    "#run sparse principal compenent analysis\n",
    "\n",
    "from time import time\n",
    "\n",
    "t0 = time()\n",
    "text_topic_spca = deco_spca.fit_transform(tm_array)\n",
    "print(\"Done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "id": "ThzsJX8Hhyoi",
    "outputId": "3df6d992-d42e-48eb-8fca-962ab9e2d7ce"
   },
   "outputs": [],
   "source": [
    "topic_words = []\n",
    "\n",
    "for topic in deco_spca.components_:\n",
    "    word_idx = np.argsort(topic)[::-1][0:num_top_words]\n",
    "    topic_words.append([vocab[i] for i in word_idx])\n",
    "\n",
    "for t in range(len(topic_words)):\n",
    "    print(\"SPCA Topic {}: {}\".format(t, ' '.join(topic_words[t][:15])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "id": "_m1LTA7Yhyo3",
    "outputId": "d1c8aa9b-641e-4858-dc4a-919102582736"
   },
   "outputs": [],
   "source": [
    "for i in range(len(text_topic_spca)):\n",
    "    top_topics = np.argsort(text_topic_spca[i,:])[::-1][0:4]\n",
    "    top_topics_str = ' '.join(str(t) for t in top_topics)\n",
    "    print(\"{}: {}\".format(titles[i], top_topics_str))         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "KubLSjxKhypX",
    "outputId": "c8967fc3-b69a-464d-a573-10f9b971f355"
   },
   "outputs": [],
   "source": [
    "deco_lda = decomposition.LatentDirichletAllocation(n_components=num_topics, \n",
    "                                                  batch_size = 64,\n",
    "                                                  max_iter=25,\n",
    "                                                  learning_method='online',\n",
    "                                                  learning_offset=1.,\n",
    "                                                  random_state=4)\n",
    "\n",
    "                                \n",
    "deco_lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "-A0PJFWghypZ",
    "outputId": "42912b84-90a5-4fc8-a1c2-d01706d800e6"
   },
   "outputs": [],
   "source": [
    "#run latent dirichlet allocation\n",
    "\n",
    "t0 = time()\n",
    "text_topic_lda = deco_lda.fit_transform(tm_array)\n",
    "print(\"Done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "id": "ZaIflF6lhypd",
    "outputId": "af779730-6054-46f8-9515-987f8bc1a6ed"
   },
   "outputs": [],
   "source": [
    "topic_words_lda = []\n",
    "\n",
    "for topic in deco_lda.components_:\n",
    "    word_idx = np.argsort(topic)[::-1][0:num_top_words]\n",
    "    topic_words_lda.append([vocab[i] for i in word_idx])\n",
    "\n",
    "for t in range(len(topic_words_lda)):\n",
    "    print(\"LDA Topic {}: {}\".format(t, ' '.join(topic_words_lda[t][:10])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "id": "8fRpChdahypj",
    "outputId": "32740265-38ff-4da4-8668-9ad2866012b4"
   },
   "outputs": [],
   "source": [
    "for i in range(len(text_topic_lda)):\n",
    "    top_topics = np.argsort(text_topic_lda[i,:])[::-1][0:4]\n",
    "    top_topics_str = ' '.join(str(t) for t in top_topics)\n",
    "    print(\"{}: {}\".format(titles[i], top_topics_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "YdKD9txwhypl",
    "outputId": "fbed397e-7853-49be-b3e8-3c29513a9a94"
   },
   "outputs": [],
   "source": [
    "deco_tsvd = decomposition.TruncatedSVD(n_components=num_topics, n_iter=50, random_state=4)\n",
    "deco_tsvd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "gdgLa8FZhypo",
    "outputId": "446867fe-76d3-47e1-ecca-818122bc9f93"
   },
   "outputs": [],
   "source": [
    "#run truncated singular value decomposition (latent semantic analysis)\n",
    "\n",
    "t0 = time()\n",
    "text_topic_tsvd = deco_tsvd.fit_transform(tm_sparse)\n",
    "print(\"Done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "id": "CLNCmlcuhypq",
    "outputId": "b6dab6ef-12d0-402b-93a7-e83edd093701"
   },
   "outputs": [],
   "source": [
    "topic_words_tsvd = []\n",
    "\n",
    "for topic in deco_tsvd.components_:\n",
    "    word_idx = np.argsort(topic)[::-1][0:num_top_words]\n",
    "    topic_words_tsvd.append([vocab[i] for i in word_idx])\n",
    "\n",
    "for t in range(len(topic_words_tsvd)):\n",
    "    print(\"TSVD Topic {}: {}\".format(t, ' '.join(topic_words_tsvd[t][:10])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "id": "w4jw3qyqhypt",
    "outputId": "c84f2583-d07a-4910-eada-e0e037d78efe"
   },
   "outputs": [],
   "source": [
    "for i in range(len(text_topic_tsvd)):\n",
    "    top_topics = np.argsort(text_topic_tsvd[i,:])[::-1][0:4]\n",
    "    top_topics_str = ' '.join(str(t) for t in top_topics)\n",
    "    print(\"{}: {}\".format(titles[i], top_topics_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "colab_type": "code",
    "id": "AwzOB3jlhyqN",
    "outputId": "baa326d3-1456-4d9e-d44e-bb8d24c8ba92"
   },
   "outputs": [],
   "source": [
    "#run term frequency - inverse document frequency and create pariwise similarity\n",
    "#matrix among the ten texts\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "documents = [open(f).read() for f in texts]\n",
    "tfidf = TfidfVectorizer(stop_words=my_stop_words).fit_transform(documents)\n",
    "pairwise_similarity = tfidf * tfidf.T\n",
    "pairwise_similarity_matrix = pairwise_similarity.todense()\n",
    "pairwise_similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 383
    },
    "colab_type": "code",
    "id": "Shjun_4hcCJY",
    "outputId": "6faa6f32-32e5-4f3b-d093-b0897bc5904f"
   },
   "outputs": [],
   "source": [
    "#convert pairwise matrix into a dataframe for easier viewing\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "psm_df = pd.DataFrame(pairwise_similarity_matrix, index = titles, columns = titles).round(3)\n",
    "psm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HOSQi0haHTyt"
   },
   "outputs": [],
   "source": [
    "#uncomment below if want to save the dataframe in a csv file\n",
    "#psm_df.to_csv('pairwiae_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LmvBOGs_HTyw"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "finTSv08.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
